{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text='''Good Morning- welcome to the text information systems cs410 test. my Name is John (twitter- @John66) and here is my friend Mark ( @MarkT88)\n",
    "John is based out of Singapore and he is reachable at 212 834 5587 . to drop an email its john.mayers@gmail.com. the website is www.johnmayers.com\n",
    "Mark is based out of London and he is reachable at 713-455-6752, email is marktom@gmail.com. He also has a website www.markT.org \n",
    "John's Singapore national id is G5443455N . Mark's used to be in Singapore once and his National ID was G5662300N .\n",
    "John's alternate phone no is +6597724501.\n",
    "Let me send you a sample of credit card info. John's ccinfo is 5520390013453413 and Mark's is 379855587611009.\n",
    "Let us see if it can find out credit card if it has dashes here is one more 5520-3877-1345-9898.\n",
    "But all in all I need call up an speak to Jane  +6237724501 \n",
    "call up Janice @ +6437724501 \n",
    "Get the problem fixed with his manager Rob = +6737724501 '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emails(text):\n",
    "        \"\"\" Returns e-mail addresses [tag: EMAIL] \"\"\"\n",
    "        emails_regex = \"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,4}\"\n",
    "        emails_re = re.compile(emails_regex)\n",
    "        emails_list = [(\"EMAIL\", email) for email in emails_re.findall(text)]\n",
    "        return emails_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phoneNumber(text, regex=None):\n",
    "        \"\"\" Returns phone numbers\"\"\"\n",
    "\n",
    "        if not regex:\n",
    "            # Using US phone regex as default\n",
    "            regex = r'''(\\b\n",
    "                                \\d{3}     # area code is 3 digits (e.g. '800')\n",
    "                                \\D*         # optional separator is any number of non-digits\n",
    "                                \\d{3}     # trunk is 3 digits (e.g. '555')\n",
    "                                \\D*         # optional separator\n",
    "                                \\d{4}\\b     # rest of number is 4 digits (e.g. '1212')\n",
    "                                )'''\n",
    "        \n",
    "        phone_re = re.compile(regex, re.VERBOSE)\n",
    "        phone_list = [(\"PHONE\", phone) for phone in phone_re.findall(text)]\n",
    "        return phone_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_urls(text):\n",
    "        \"\"\" Returns URLs\"\"\"\n",
    "        url_regex = r'''\n",
    "                        (?xi)\n",
    "                            \\b\n",
    "                            (                           \n",
    "                            (?:\n",
    "                                [a-z][\\w-]+:                \n",
    "                                (?:\n",
    "                                /{1,3}                        \n",
    "                                |                             \n",
    "                                [a-z0-9%]                     \n",
    "                                                                \n",
    "                                )\n",
    "                                |                           \n",
    "                                www\\d{0,3}[.]               \n",
    "                                |                           \n",
    "                                [a-z0-9.\\-]+[.][a-z]{2,4}/  \n",
    "                            )\n",
    "                            (?:                           \n",
    "                                [^\\s()<>]+                      \n",
    "                                |                               \n",
    "                                \\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)  \n",
    "                            )+\n",
    "                            (?:                           \n",
    "                                \\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)  \n",
    "                                |                                   \n",
    "                                [^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]\n",
    "                            )\n",
    "                        )'''\n",
    "\n",
    "        url_re = re.compile(url_regex, re.VERBOSE)\n",
    "        url_list = [(\"URL\", url[0]) for url in url_re.findall(text)]\n",
    "        return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_singaporeID(text):\n",
    "        \"\"\"Returns Singapore National ID based on  characters and length long\"\"\"\n",
    "        ids_list = []\n",
    "        ends=['N','S','R']\n",
    "        starts=['G','S']\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if len(word) >=8 and any(word.startswith(start) for start in starts) and any(word.endswith(end) for end in ends) and any(char.isdigit() for char in word):\n",
    "                    ids_list.append((\"Singapore ID\", word))\n",
    "        return ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ids(text):\n",
    "        \"\"\"Returns IDs based on length - needs work to make it accurate\"\"\"\n",
    "        ids_list = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if len(word) >= 4 and len(word)<=8 and any(char.isdigit() for char in word):\n",
    "                    ids_list.append((\"ID\", word))\n",
    "        return ids_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_twitterID(text):\n",
    "        \"\"\"Returns Twitter usernames\n",
    "        \"\"\"\n",
    "        twitter_regex = r'^|[^@\\w](@\\w{1,15})\\b'\n",
    "        twitter_re = re.compile(twitter_regex)\n",
    "        twitter_list = [(\"TWITTER\", twitter) for twitter in twitter_re.findall(\n",
    "            text) if twitter != \"\"]\n",
    "        return twitter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_chunks(text, label):\n",
    "    chunked = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == nltk.Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_location(text,label='GPE'):\n",
    "    loc_list = [('Location', loc ) for loc in get_chunks(text, label)]\n",
    "    return loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_creditCard(text):\n",
    "        \"\"\"Returns CC Info based on length - can also find out if its AMEX or not\"\"\"\n",
    "        cc_regex=r'''(\\b(?:\\d[ -]*?){13,16}\\b)'''\n",
    "        cc_re = re.compile(cc_regex, re.VERBOSE)\n",
    "        cc_list=[]\n",
    "        for ccInfo in cc_re.findall(text):\n",
    "            if len(ccInfo) ==15:\n",
    "                cc_list.append((\"CreditCard-AMEX\", ccInfo))\n",
    "            else:\n",
    "                cc_list.append((\"CreditCard\", ccInfo))\n",
    "        return cc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sensitive_data(text, **kwargs):\n",
    "        \"\"\" Returns sensitive info\n",
    "        \"\"\"\n",
    "        return  find_singaporeID(text)+  \\\n",
    "                find_twitterID(text) + \\\n",
    "                find_emails(text) + \\\n",
    "                find_urls(text) + \\\n",
    "                find_phoneNumber(text)+ \\\n",
    "                find_location(text)+ \\\n",
    "                find_creditCard(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-b25a1d2de9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline_raw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf_sens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mv_line_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_singaporeID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0ms_singaporeID\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_singaporeID\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xef in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "# List of files to be processed\n",
    "arr_file_list = [\"output_10_oliver_twist.txt\",\n",
    "\"output_09_ulysses.txt\",\"output_06_moby_dick.txt\",\"output_03_hamlet.txt\",\n",
    "\"output_08_iliad.txt\",\"output_05_christmas_carol.txt\",\"output_02_romeo_juliet.txt\",\"output_07_dracula.txt\"]\n",
    "# \"output_04_kant.txt\",\"output_01_macbeth.txt\",\n",
    "\n",
    "v_doc_count = 0\n",
    "# output file\n",
    "f_run_log = open('run_log.txt','w')\n",
    "\n",
    "# 1. loop through each file\n",
    "# 2. Analize the POS tags\n",
    "# 3. Evaluate the risk of each word\n",
    "for _file in arr_file_list: \n",
    "    \n",
    "    v_doc_count  += 1\n",
    "    \n",
    "    v_file_in = 'files/'+_file\n",
    "    v_file_out = 'files/output_'+_file\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Initialize variables to collect and print content\n",
    "    s_singaporeID =  set([])\n",
    "    s_twitterID =  set([])\n",
    "    s_emails =  set([])\n",
    "    s_urls =  set([])\n",
    "    s_phoneNumber =  set([])\n",
    "    s_location =  set([])\n",
    "    s_creditCard =  set([])\n",
    "    v_line_count = 0\n",
    "    v_singaporeID_count = 0\n",
    "    v_twitterID_count = 0\n",
    "    v_emails_count = 0\n",
    "    v_urls_count = 0\n",
    "    v_phoneNumber_count = 0\n",
    "    v_location_count = 0\n",
    "    v_creditCard_count = 0\n",
    "    \n",
    "    f_sens = open(v_file_in,'r')\n",
    "    \n",
    "    \n",
    "    for line_raw in f_sens:\n",
    "        v_line_count += 1\n",
    "        line = line_raw.encode(\"utf-16\")\n",
    "        if len(find_singaporeID(line))>0:\n",
    "            s_singaporeID.add(str(find_singaporeID(line)))\n",
    "            v_singaporeID_count += 1\n",
    "        if len(find_twitterID(line))>0:\n",
    "            s_twitterID.add(str(find_twitterID(line)))\n",
    "            v_twitterID_count += 1\n",
    "        if len(find_emails(line))>0:\n",
    "            s_emails.add(str(find_emails(line)))\n",
    "            v_emails_count += 1\n",
    "        if len(find_urls(line))>0:\n",
    "            s_urls.add(str(find_urls(line)))\n",
    "            v_urls_count += 1\n",
    "        if len(find_phoneNumber(line))>0:\n",
    "            s_phoneNumber.add(str(find_phoneNumber(line)))\n",
    "            v_phoneNumber_count += 1\n",
    "        if len(find_location(line))>0:\n",
    "            s_location.add(str(find_location(line)))\n",
    "            v_location_count += 1\n",
    "        if len(find_creditCard(line))>0:\n",
    "            s_creditCard.add(str(find_creditCard(line)))\n",
    "            v_creditCard_count += 1   \n",
    "    f_sens.close()\n",
    "    f_run_log.write(\"Document number being processed : \" + str(v_doc_count))\n",
    "    f_run_log.write(\"\\n\\nTotal number of lines in the file : \" + str(v_line_count))\n",
    "    f_run_log.write(\"\\n\\nTotal number occurrences of Singaporean IDs in the file : \" + str(v_singaporeID_count))\n",
    "    f_run_log.write((\"====> The Singapore IDs are \" + str(s_singaporeID)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences Twitter IDs in the file : \" + str(v_twitterID_count))\n",
    "    f_run_log.write((\"====> The Twitter IDs are \" + str(s_twitterID)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences Email IDs in the file : \" + str(v_emails_count))\n",
    "    f_run_log.write((\"====> The Email IDs are \" + str(s_emails)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences URLs in the file : \" + str(v_urls_count))\n",
    "    f_run_log.write((\"====> The URLs are \" + str(s_urls)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences Ph Numbers in the file : \" + str(v_phoneNumber_count))\n",
    "    f_run_log.write((\"====> The Phone numbers are \" + str(s_phoneNumber)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences Addresses in the file : \" + str(v_location_count))\n",
    "    f_run_log.write((\"====> The Addresses are \" + str(s_location)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n\\nTotal number of occurrences Credit Cards in the file : \" + str(v_creditCard_count))\n",
    "    f_run_log.write((\"====> The credit-card numbers are \" + str(s_creditCard)).encode(\"utf-8\"))\n",
    "    f_run_log.write(\"\\n$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\\n\")\n",
    "v_doc_count = 0\n",
    "f_run_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
